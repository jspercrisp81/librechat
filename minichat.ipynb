{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fd061aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
    "from typing import List, Optional\n",
    "from typing import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02b0e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"COHERE_API_KEY\"] = \"Bl5kPeaI9IELAiPwj3B72o5Ub95xoJE9teskkKdI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f285683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = CohereEmbeddings(model=\"embed-multilingual-light-v3.0\", max_retries=5, request_timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa5d8fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WikipediaDocumentProcessor:\n",
    "    \"\"\"\n",
    "    A class to load and recursively split documents from Wikipedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, queries: List[str], load_max_docs: int = 1, doc_content_chars_max=100000, lang: str = 'fa'):\n",
    "        \"\"\"\n",
    "        Initialize the Wikipedia document processor.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of search queries for Wikipedia\n",
    "            load_max_docs: Maximum number of documents to load per query (default: 1)\n",
    "            lang: Language code for Wikipedia (default: 'fa' for Persian)\n",
    "        \"\"\"\n",
    "        self.queries = queries\n",
    "        self.load_max_docs = load_max_docs\n",
    "        self.lang = lang\n",
    "        self.documents: List[Document] = []\n",
    "        self.split_documents: List[Document] = []\n",
    "        self.doc_content_chars_max = doc_content_chars_max\n",
    "        \n",
    "        # Initialize text splitter for recursive splitting\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        # Load documents immediately upon initialization\n",
    "        self.load_documents()\n",
    "    \n",
    "    def load_documents(self) -> None:\n",
    "        \"\"\"\n",
    "        Load documents from Wikipedia for each query.\n",
    "        \"\"\"\n",
    "        print(f\"Loading Wikipedia documents in '{self.lang}' language...\")\n",
    "        \n",
    "        for query in self.queries:\n",
    "            try:\n",
    "                print(f\"Loading document for query: '{query}'\")\n",
    "                \n",
    "                # Load document from Wikipedia\n",
    "                loader = WikipediaLoader(\n",
    "                    query=query,\n",
    "                    load_max_docs=self.load_max_docs,\n",
    "                    lang=self.lang,\n",
    "                    doc_content_chars_max=self.doc_content_chars_max,\n",
    "                )\n",
    "                docs = loader.load()\n",
    "                self.documents.extend(docs)\n",
    "                print(f\"âœ“ Successfully loaded {len(docs)} document(s) for '{query}'\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error loading document for '{query}': {e}\")\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            metadata = doc.metadata\n",
    "            new_metadata = {\n",
    "                'title': metadata['title'],\n",
    "                'language': self.lang,\n",
    "            }\n",
    "            self.documents[i].metadata = new_metadata\n",
    "    \n",
    "    def split_documents_recursively(self, chunk_size: int = 1000, chunk_overlap: int = 200) -> List[str]:\n",
    "        \"\"\"\n",
    "        Recursively split the loaded documents into smaller chunks.\n",
    "        \n",
    "        Args:\n",
    "            chunk_size: Size of each chunk (default: 1000)\n",
    "            chunk_overlap: Overlap between chunks (default: 200)\n",
    "            \n",
    "        Returns:\n",
    "            List of split document chunks\n",
    "        \"\"\"\n",
    "        if not self.documents:\n",
    "            print(\"No documents loaded to split.\")\n",
    "            return []\n",
    "        \n",
    "        # Update text splitter parameters\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        print(\"Splitting documents recursively...\")\n",
    "        \n",
    "        # Split all documents\n",
    "        self.split_documents = self.text_splitter.split_documents(self.documents)\n",
    "        \n",
    "        print(f\"âœ“ Successfully split {len(self.documents)} documents into {len(self.split_documents)} chunks\")\n",
    "    \n",
    "    def get_loaded_documents(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get the content of loaded documents.\n",
    "        \n",
    "        Returns:\n",
    "            List of document contents\n",
    "        \"\"\"\n",
    "        return [doc.page_content for doc in self.documents]\n",
    "    \n",
    "    def get_split_documents(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get the content of split documents.\n",
    "        \n",
    "        Returns:\n",
    "            List of split document chunks\n",
    "        \"\"\"\n",
    "        return [doc.page_content for doc in self.split_documents]\n",
    "    \n",
    "    def get_document_metadata(self) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Get metadata of loaded documents.\n",
    "        \n",
    "        Returns:\n",
    "            List of document metadata\n",
    "        \"\"\"\n",
    "        return [doc.metadata for doc in self.documents]\n",
    "    \n",
    "    def print_summary(self) -> None:\n",
    "        \"\"\"\n",
    "        Print a summary of loaded and split documents.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"DOCUMENT PROCESSING SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Queries processed: {len(self.queries)}\")\n",
    "        print(f\"Documents loaded: {len(self.documents)}\")\n",
    "        print(f\"Chunks created: {len(self.split_documents)}\")\n",
    "        \n",
    "        if self.documents:\n",
    "            print(f\"\\nOriginal document sizes:\")\n",
    "            for i, doc in enumerate(self.documents):\n",
    "                print(f\"  Document {i+1}: {len(doc.page_content)} characters\")\n",
    "        \n",
    "        if self.split_documents:\n",
    "            print(f\"\\nChunk sizes:\")\n",
    "            for i, chunk in enumerate(self.split_documents):\n",
    "                print(f\"  Chunk {i+1}: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14195729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia documents in 'fa' language...\n",
      "Loading document for query: 'Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†'\n",
      "âœ“ Successfully loaded 1 document(s) for 'Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†'\n",
      "Splitting documents recursively...\n",
      "âœ“ Successfully split 1 documents into 25 chunks\n"
     ]
    }
   ],
   "source": [
    "titles = ['Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†']\n",
    "wikiloader = WikipediaDocumentProcessor(queries=titles)\n",
    "wikiloader.split_documents_recursively()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad11bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(\n",
    "    documents = wikiloader.split_documents, # The documents to embed\n",
    "    embedding = embeddings, # The embeddings to use\n",
    "    collection_name = \"librechat_wiki\", # The name of the collection\n",
    "    persist_directory = \"./collections\", # Where to save the collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf02cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: Û±Û¹Û¸Ûµ: Ø§Ùˆ Ø¨Ù†ÛŒØ§Ø¯ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ÛŒ Ø±Ø§ Ø¢ØºØ§Ø² Ú©Ø±Ø¯ Ú©Ù‡ Ø®ÙˆØ¯Ø´ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ø¦ÛŒØ³ Ø¯Ø§ÙˆØ·Ù„Ø¨Ø§Ù†Ù‡Ù” ØªÙ…Ø§Ù… ÙˆÙ‚Øª Ø¢Ù† Ø¨ÙˆØ¯.\n",
      "Ø³ÛŒØ³ØªÙ… Ú¯Ù†Ùˆ/ Ù„ÛŒÙ†ÙˆÚ©Ø³ Ú©Ù‡ Ù‡Ù…Ú†Ù†ÛŒÙ† Ù‡Ø³ØªÙ‡Ù” Ù„ÛŒÙ†ÙˆÚ©Ø³ ØªÙˆØ³Ø¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ù„ÛŒÙ†ÙˆØ³ ØªÙˆØ±ÙˆØ§Ù„Ø¯Ø² (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: Linus Torvalds) Ø±Ø§ Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ¨Ø±Ø¯ØŒ Ø¯Ø± Ø±Ø§ÛŒØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø±ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´Ø¯ Ùˆ Ù‡Ù…â€ŒØ§Ú©Ù†ÙˆÙ† Ø¯Ø± ÙØ±ÙˆØ´Ú¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© Ø±Ø§ÛŒØ§Ù†Ù‡ Ù†ÛŒØ² Ø¨Ù‡ ØµÙˆØ±Øª Ø§Ø² Ù¾ÛŒØ´ Ù†ØµØ¨ Ø´Ø¯Ù‡ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒØ¨Ø§Ø´Ù†Ø¯. Ù‡Ø± Ú†Ù†Ø¯ ØªÙˆØ²ÛŒØ¹â€ŒÚ©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø§ÛŒÙ† Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ Ú¯Ø§Ù‡ÛŒ ØªÙÚ©Ø± Ø¢Ø²Ø§Ø¯ÛŒ Ø±Ø§ Ú©Ù‡ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ Ø±Ø§ Ù…Ù‡Ù… Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ØŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ†Ø¯ØŒ Ùˆ Ø­ØªÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±Ù‡Ø§ÛŒ ØºÛŒØ± Ø¢Ø²Ø§Ø¯ Ø±Ø§ Ø¯Ø± Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒØ´Ø§Ù† Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù†Ø¯. Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø§Ø³Øª Ú©Ù‡ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø§Ø² Ø§ÙˆØ§Ø³Ø· Ø¯Ù‡Ù‡Ù” Û±Û¹Û¹Û° Ù…ÛŒÙ„Ø§Ø¯ÛŒØŒ Ø¨ÛŒØ´ØªØ± ÙˆÙ‚Øª Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø­Ù…Ø§ÛŒØª Ø³ÛŒØ§Ø³ÛŒ Ø§Ø² Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ Ùˆ Ú¯Ø³ØªØ±Ø´ ØªÙÚ©Ø±Ø§Øª Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø§ÛŒÙ† Ø¬Ù†Ø¨Ø´ ØµØ±Ù Ú©Ø±Ø¯ØŒ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø¨Ø§Ø±Ø² Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± Ø«Ø¨Øª Ø§Ø®ØªØ±Ø§Ø¹ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªÙˆØ³Ø¹Ù‡ Ø®Ø·Ø±Ù†Ø§Ú© Ø­Ù‚ Ù†Ø´Ø± (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: Copyright) Ø¨ÙˆØ¯.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ù…ØªÛŒÙˆ Ø§Ø³ØªØ§Ù„Ù…Ù† (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: Richard Matthew Stallman) (Ù…ØªÙˆÙ„Ø¯ Û±Û¶ Ù…Ø§Ø±Ø³ Û±Û¹ÛµÛ³ Ø¯Ø± Ù†ÛŒÙˆÛŒÙˆØ±Ú©) Ù…Ø¹Ø±ÙˆÙ Ø¨Ù‡ Ø¢Ø±-Ø§Ù…-Ø§Ø³ (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: RMS)ØŒ ÛŒÚ© Ø¢Ù…Ø±ÛŒÚ©Ø§ÛŒÛŒ Ø·Ø±ÙØ¯Ø§Ø± Ø¢Ø²Ø§Ø¯ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ± Ø§Ø³Øª. Ø¯Ø± Ø³Ù¾ØªØ§Ù…Ø¨Ø± Ø³Ø§Ù„ Û±Û¹Û¸Û³ØŒ Ø§Ùˆ Ù¾Ø±ÙˆÚ˜Ù‡ Ú¯Ù†Ùˆ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª ÛŒÚ© Ø³ÛŒØ³ØªÙ…â€ŒØ¹Ø§Ù…Ù„ Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¢Ø²Ø§Ø¯ Ø´Ø¨Ù‡-ÛŒÙˆÙ†ÛŒÚ©Ø³ Ø¢ØºØ§Ø² Ú©Ø±Ø¯ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ùˆ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ Ø¹Ù‡Ø¯Ù‡â€ŒØ¯Ø§Ø± Ø´Ø¯. Ø¨Ø§ Ø¢ØºØ§Ø² Ù¾Ø±ÙˆÚ˜Ù‡ Ú¯Ù†ÙˆØŒ Ø§Ùˆ Ù†Ø®Ø³ØªÛŒÙ† Ù‚Ø¯Ù… Ø±Ø§ Ø¯Ø± Ø¬Ù†Ø¨Ø´ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ Ø¨Ø±Ø¯Ø§Ø´Øª Ùˆ Ø¯Ø± Ø§Ú©ØªØ¨Ø± Ø³Ø§Ù„ Û±Û¹Û¸Ûµ Ø¨Ù†ÛŒØ§Ø¯ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ Ø±Ø§ ØªØ£Ø³ÛŒØ³ Ú©Ø±Ø¯.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: === Ø§ØªÙØ§Ù‚Ø§Øª Ù…Ù‡Ù… Ø²Ù†Ø¯Ú¯ÛŒ ===\n",
      "Û±Û¹Û¸Û³: Ø§Ø³ØªØ§Ù„Ù…Ù† Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙˆØ³Ø¹Ù‡ Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„ Ú¯Ù†ÙˆØŒ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„ Ø´Ø¨Ù‡â€ŒÛŒÙˆÙ†ÛŒÚ©Ø³ØŒ Ø¨Ù‡ ÙˆØ¬ÙˆØ¯ Ø¢Ù…Ø¯Ù‡ Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ú©Ø§Ù…Ù„Ø§Ù‹ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ÛŒ Ø¨Ø§Ø´Ø¯ Ùˆ Ø§Ø² Ø¢Ù† Ù¾Ø³ Ø±Ù‡Ø¨Ø± Ø¨Ù†ÛŒØ§Ø¯ Ø´Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ùˆ Ø¨Ø§ Ø§ÛŒÙ† Ø§Ø¹Ù„Ø§Ù…ÛŒÙ‡ØŒ Ø¬Ù†Ø¨Ø´ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ Ø±Ø§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ø±Ø¯.\n",
      "Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¯Ø± Ûµ Ú˜Ø§Ù†ÙˆÛŒÙ‡ Û±Û¹Û¸Û´ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯Ù† Ø±ÙˆÛŒ Ø§ÛŒÙ† Ø¨Ù†ÛŒØ§Ø¯ Ú©Ø±Ø¯ Ø§Ø² Ú©Ø§Ø±Ú¯Ø²ÛŒÙ†ÛŒ Ø¯Ø± MIT Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø§Ù†Ø¬Ø§Ù… Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ø³ØªØ¹ÙØ§Ø¡ Ø¯Ø§Ø¯Ù‡ Ø¨ÙˆØ¯.\n",
      "Û±Û¹Û¸Ûµ: Ø§Ùˆ Ø¨Ù†ÛŒØ§Ø¯ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ÛŒ Ø±Ø§ Ø¢ØºØ§Ø² Ú©Ø±Ø¯ Ú©Ù‡ Ø®ÙˆØ¯Ø´ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø±Ø¦ÛŒØ³ Ø¯Ø§ÙˆØ·Ù„Ø¨Ø§Ù†Ù‡Ù” ØªÙ…Ø§Ù… ÙˆÙ‚Øª Ø¢Ù† Ø¨ÙˆØ¯.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = 'Ú†Ù‡ Ú©Ø³ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±Ù‡Ø§ÛŒ Ø¢Ø²Ø§Ø¯ Ø±Ø§ Ø¨Ù†Ø§ Ù†Ù‡Ø§Ø¯ØŸ'\n",
    "\n",
    "results = vector_store.similarity_search(query, k = 3)\n",
    "\n",
    "for result in results:\n",
    "    print('Documnt Title:', result.metadata['title'])\n",
    "    print('Document Content:', result.page_content)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8d38033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: == Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ÛŒ Ù…Ù†ØªÙ‡ÛŒ Ø¨Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ú¯Ù†Ùˆ ==\n",
      "Ø¯Ø± Ø§ÙˆØ§Ø®Ø± Ø¯Ù‡Ù‡ Û±Û¹Û·Û° Ùˆ Ø§ÙˆØ§ÛŒÙ„ Ø¯Ù‡Ù‡ Û±Û¹Û¸Û°ØŒ ÙØ±Ù‡Ù†Ú¯ Ù‡Ú©Ø±Ù‡Ø§ Ú©Ù‡ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¨Ù‡ Ø¢Ù† Ø±ÙˆÙ†Ù‚ Ø¯Ø§Ø¯Ù‡ Ø¨ÙˆØ¯ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ù…ØªÙ„Ø§Ø´ÛŒ Ø´Ø¯Ù† Ú©Ø±Ø¯. Ø®ÛŒÙ„ÛŒ Ø§Ø² ØªÙˆÙ„ÛŒØ¯Ú©Ù†Ù†Ø¯Ú¯Ø§Ù† Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ú©Ø±Ø¯Ù† Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù† Ø¯Ø± Ø±Ø§ÛŒØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø±Ù‚Ø¨Ø§ÛŒØ´Ø§Ù†ØŒ ØªÙˆØ²ÛŒØ¹ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ù…Ù†Ø¨Ø¹ Ø±Ø§ Ù…ØªÙˆÙ‚Ù Ú©Ø±Ø¯Ù†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ùˆ Ù…Ù…Ù†ÙˆØ¹ Ø³Ø§Ø®ØªÙ† Ú©Ù¾ÛŒ Ùˆ ØªÙˆØ²ÛŒØ¹ Ù…Ø¬Ø¯Ø¯ØŒ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©Ù¾ÛŒ Ø±Ø§ÛŒØª Ùˆ Ù…Ø¬ÙˆØ²Ù‡Ø§ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù†Ø¯. Ø§ÛŒÙ†Ú†Ù†ÛŒÙ† Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±Ù‡Ø§ÛŒ Ø§Ø®ØªØµØ§ØµÛŒ Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ†Ø¯ØŒ Ùˆ Ø§ÛŒÙ† Ø§Ù…Ø± Ø§ÛŒÙ†Ø·ÙˆØ± Ø¸Ø§Ù‡Ø± Ø´Ø¯ Ú©Ù‡ Ø¨ØªÙˆØ§Ù†Ø¯ Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø±Ø§Ø³ØªØ§ Ø´ÙˆØ¯.\n",
      "Ø¶Ø±ÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ø± Ø±ÙˆÛŒ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø§Ø´ÛŒÙ†Ù‡Ø§ Ùˆ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØª Ú©Ø§Ù…Ù„Ø§Ù‹ Ù…Ø´Ù‡ÙˆØ¯ Ø¨ÙˆØ¯ØŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ØªÙˆØ§Ù†Ù†Ø¯ Ø±ÙˆÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØª Ø§Ø¬Ø±Ø§ Ø´ÙˆÙ†Ø¯ Ùˆ Ø§Ù…Ú©Ø§Ù† Ø§Ø´ØªØ±Ø§Ú© Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ù…Ø§Ø¨ÛŒÙ† Ú†Ù†Ø¯ Ø³ÛŒØ³ØªÙ… Ù…ØªÙØ§ÙˆØª Ø¨Ù‡ ÙˆØ¬ÙˆØ¯ Ø¢ÙˆØ±Ù†Ø¯. Ø§ÛŒÙ† Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø±Ø²Ø´ Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø± Ø±Ø§ÛŒØ§Ù†Ù‡ Ùˆ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¯Ø§Ø´ØªØŒ Ø§ÛŒÙ† Ù†Ú©ØªÙ‡ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø³Ø§Ø²Ù†Ø¯Ú¯Ø§Ù† Ø±Ø§ÛŒØ§Ù†Ù‡â€ŒÙ‡Ø§ Ù‡Ù… Ù…Ø´Ù‡ÙˆØ¯ Ø¨ÙˆØ¯. Ø¯Ø± Ø§ÛŒÙ† Ø­Ø§Ù„ Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯ Ø¨Ø§ Ø¨Ù‡ ÙˆØ¬ÙˆØ¯ Ø¢ÙˆØ±Ø¯Ù† Ø§Ù†Ø­ØµØ§Ø± Ùˆ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªØŒ Ø§Ø² Ú©Ù¾ÛŒ Ø´Ø¯Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ú©Ù†Ù†Ø¯ Ùˆ ÙØ±ØµØª Ø±Ø§ Ø§Ø² Ø±Ù‚ÛŒØ¨ Ø®ÙˆØ¯ Ø¨Ú¯ÛŒØ±Ù†Ø¯.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: == Ù„ÙØ¸â€ŒÚ¯Ø°Ø§Ø±ÛŒ ==\n",
      "Ø§Ø³ØªØ§Ù„Ù…Ù† Ø±ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ùˆ Ø¬Ù…Ù„Ø§ØªÛŒ Ú©Ù‡ Ø§Ø² Ø³ÙˆÛŒ Ù…Ø±Ø¯Ù… Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ø±Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø·Ø±Ø² Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ø±Ø¯Ù† Ø¢Ù†â€ŒÙ‡Ø§ Ø­Ø³Ø§Ø³ÛŒØª Ø®Ø§ØµÛŒ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡â€ŒØ§Ø³Øª. Ø¨Ù‡ Ø®ØµÙˆØµ Ø±Ø§Ø¨Ø·Ù‡ Ù…Ø§Ø¨ÛŒÙ† Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ùˆ Ø¢Ø²Ø§Ø¯ÛŒ. Ø§Ùˆ Ø¨Ù‡ Ø·Ø±Ø² Ø®Ø³ØªÚ¯ÛŒ Ù†Ø§Ù¾Ø°ÛŒØ±ÛŒ Ø§Ø² Ù…Ø±Ø¯Ù… Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯ Ú©Ù‡ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ (free software) Ùˆ Ú¯Ù†Ùˆ/Ù„ÛŒÙ†ÙˆÚ©Ø³ Ø±Ø§ Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ø¨Ø±Ù†Ø¯ Ùˆ Ø§Ø² Ù‚ÙˆØ§Ù†ÛŒÙ† Ùˆ Ù…Ù‚Ø±Ø±Ø§Øª Ú©Ù¾ÛŒ Ø±Ø§ÛŒØª Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†Ù†Ø¯. Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù…Ù†Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ú¯Ù†Ùˆ/Ù„ÛŒÙ†ÙˆÚ©Ø³ Ùˆ Ù…ØªÙ†â€ŒØ¨Ø§Ø²ØŒ Ø±ÙˆÛŒ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù† ØµØ­ÛŒØ­ Ú©Ù„Ù…Ø§Øª Ùˆ Ø¨Ù‡ Ø®ØµÙˆØµ Ø¯Ø±Ú© Ù…Ø¹Ù†Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø§ØµØ±Ø§Ø± Ù…ÛŒâ€ŒÙˆØ±Ø²Ø¯.\n",
      "ÛŒÚ©ÛŒ Ø§Ø² Ø¶ÙˆØ§Ø¨Ø· Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¨Ø±Ø§ÛŒ Ù…ØµØ§Ø­Ø¨Ù‡ Ø¯Ø§Ø¯Ù† Ø§ÛŒÙ†Ø³Øª Ú©Ù‡ Ø¢Ù† Ø±ÙˆØ²Ù†Ø§Ù…Ù‡â€ŒÙ†Ú¯Ø§Ø± Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ùˆ Ø¯Ø± Ù…Ù‚Ø§Ù„Ù‡â€ŒØ§Ø´ Ù…ÙˆØ§ÙÙ‚ Ø¨Ø§Ø´Ø¯. Ø§Ùˆ Ø­ØªÛŒ Ø¨Ø¹Ø¶ÛŒ ÙˆÙ‚ØªÙ‡Ø§ Ø¶Ø±ÙˆØ±ÛŒ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯ Ú©Ù‡ Ø±ÙˆØ²Ù†Ø§Ù…Ù‡â€ŒÙ†Ú¯Ø§Ø±Ø§Ù† Ù‚Ø¨Ù„ Ø§Ø² Ù…ØµØ§Ø­Ø¨Ù‡ØŒ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¨Ù‡Ø±Ù‡â€ŒÙˆØ±ÛŒØŒ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² ÙÙ„Ø³ÙÙ‡ Ú¯Ù†Ùˆ Ø±Ø§ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†Ù†Ø¯. Ø§Ù„Ø¨ØªÙ‡ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù†Ú©ØªÙ‡ Ù…Ø«Ø¨Øª Ø¨Ø±Ø§ÛŒ Ø§Ùˆ Ù…Ø­Ø³ÙˆØ¨ Ø´Ø¯Ù‡â€ŒØ§Ø³Øª Ùˆ Ø·Ø±ÙØ¯Ø§Ø±Ø§Ù† Ø®Ø§Øµ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø§Ø±Ø¯ØŒ Ø²ÛŒØ±Ø§ Ø¯Ø± Ù…ÙˆØ§Ø²Ø§Øª Ù…ØµØ§Ø­Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ±Ø§ÙˆØ§Ù† Ùˆ Ø¬Ø°Ø§Ø¨ØŒ Ø§Ø² Ù…Ø¨Ø§Ø­Ø«Ù‡ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø¨Ø¹Ø¶ÛŒ Ø§Ø² Ù…ÙˆØ§Ø±Ø¯ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: Ø¯Ø± Ø¯Ù‡Ù‡ Û±Û¹Û¸Û° Ø±ÛŒÚ†Ø§Ø±Ø¯ Ú¯Ø±ÛŒÙ†Ø¨Ù„Øª Ù‡Ú©Ø± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒØŒ Ø´Ø±Ú©Øª Lisp Machines Ø±Ø§ Ø¨Ù‡ Ø«Ø¨Øª Ø±Ø³Ø§Ù†Ø¯ Ùˆ Ø§ÛŒÙ† Ù…Ø§Ø´ÛŒÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø¨Ø§Ø²Ø§Ø± Ø§Ø±Ø§Ø¦Ù‡ Ú©Ø±Ø¯. Ú©Ø§Ø±ÛŒ Ú©Ù‡ Ø§Ùˆ Ùˆ ØªØ§Ù… Ù†Ø§ÛŒØª Ø¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯Ù‡ Ø¨ÙˆØ¯Ù†Ø¯. Ú¯Ø±ÛŒÙ†Ø¨Ù„Øª Ú©Ù…Ú©Ù‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ Ø±Ø§ Ø±Ø¯ Ú©Ø±Ø¯. ÙˆÛŒ Ù…Ø¹ØªÙ‚Ø¯ Ø¨ÙˆØ¯ Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø§ ÙØ±ÙˆØ´ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø² Ù…Ø§Ø´ÛŒÙ†Ù‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù‚Ø¯Ø±Øª Ù…Ø§Ù„ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ø±Ù‡ Ùˆ Ú¯Ø³ØªØ±Ø´ Ø´Ø±Ú©Øª Ø±Ø§ Ø¨Ù‡â€ŒØ¯Ø³Øª Ø¢ÙˆØ±Ù†Ø¯.\n",
      "Ø¢Ø± Ø§Ù… Ø§Ø³ Ø§Ø² Ø³Ø§Ù„ Û±Û¹Û¸Û² ØªØ§ Û±Û¹Û¸Û³ Ø¨Ù‡ Ù…Ø¯Øª Ø¯Ùˆ Ø³Ø§Ù„ Ø§Ø² Ø§Ù†Ø­ØµØ§Ø±ÛŒ Ø´Ø¯Ù† Symbolics Ø±ÙˆÛŒ Ø±Ø§ÛŒØ§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ ØªÙˆØ³Ø· Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³Ø§Ù† Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø²Ù…Ø§Ù†ÛŒ Ø¨ÙˆØ¯ Ú©Ù‡ Ø§Ùˆ Ø¢Ø®Ø±ÛŒÙ† ØªÙ„Ø§Ø´Ù‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¬Ù‡Øª ØªÙ‚ÙˆÛŒØª Ù‡Ú©Ø±Ù‡Ø§ Ø¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ ØµØ±Ù Ù…ÛŒâ€ŒÚ©Ø±Ø¯. Ø§Ùˆ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ ÛŒÚ© Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯ Ø¨Ø§Ø² Ø¨ÙˆØ¯ Ú©Ù‡ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ùˆ Ø§Ù†Ø­ØµØ§Ø± Ø±Ø§ Ø¯Ø± Ø®ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ Ø§Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ø² Ø¢Ù† Ø¨Ù‡ ÙÚ©Ø± Ø§ÙØªØ§Ø¯ ØªØ§ Ø§ÛŒÙ† Ø¹Ù‚ÛŒØ¯Ù‡ Ø±Ø§ Ø¨Ø§ Ø¯ÛŒÚ¯Ø±Ø§Ù† Ø¨Ù‡ Ø§Ø´ØªØ±Ø§Ú© Ø¨Ú¯Ø°Ø§Ø±Ø¯ Ùˆ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ù†Ø¬Ø§Ù… Ú©Ø§Ø± Ø¨Ù‡ Ù…Ø§Ù†Ù†Ø¯ ÛŒÚ© Ø¯Ø§Ù†Ø´Ù…Ù†Ø¯ Ø§ØµÛŒÙ„ Ø§ÛŒÙ† Ø§ÛŒØ¯Ù‡ Ø±Ø§ Ø¨Ø§ Ø¯ÛŒÚ¯Ø±Ø§Ù† Ø¨Ù‡ Ø§Ø´ØªØ±Ø§Ú© Ø¨Ú¯Ø°Ø§Ø±Ø¯. Ø¯Ø§Ù†Ø´Ù…Ù†Ø¯Ø§Ù† Ø¨Ø¯ÙˆÙ† Ù‡ÛŒÚ† Ø®Ø³Ø§Ø³ØªÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ùˆ Ø¯Ø§Ù†Ø´ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ø¯ÛŒÚ¯Ø±Ø§Ù† ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ùˆ Ø¯Ø± Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ØªØ§ÛŒØ¬ Ùˆ Ø§Ù†Ø¬Ø§Ù… Ú©Ø§Ø±Ø§Ù‡Ø§ Ø¨Ù‡ Ø¯ÛŒÚ¯Ø±Ø§Ù† Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù„Ø²ÙˆÙ… Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Ø¯Ø± Ø¨ÛŒØ§Ù†ÛŒÙ‡â€ŒÛŒ Ù‡Ú©Ø±Ù‡Ø§ Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ú©Ù‡ Ø¬Ø±Ù… Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ú©Ù„Ù…Ù‡ Ú†ÛŒØ³ØªØŸ\"\n",
    "\n",
    "results = vector_store.similarity_search(query, k = 3)\n",
    "\n",
    "for result in results:\n",
    "    print('Documnt Title:', result.metadata['title'])\n",
    "    print('Document Content:', result.page_content)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e2ad9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: == Ø²Ù†Ø¯Ú¯ÛŒ ==\n",
      "Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¯Ø± Ø³Ø§Ù„ Û±Û¹ÛµÛ³ Ø¯Ø± Ø´Ù‡Ø± Ù†ÛŒÙˆÛŒÙˆØ±Ú© Ø¨Ù‡ Ø¯Ù†ÛŒØ§ Ø¢Ù…Ø¯. Alice Lippman Ùˆ Daniel Stallman Ù¾Ø¯Ø± Ùˆ Ù…Ø§Ø¯Ø± Ø§Ùˆ Ø¨ÙˆØ¯Ù†Ø¯. Ø§ÙˆÙ„ÛŒÙ† ØªØ¬Ø±Ø¨Ù‡ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¨Ø§ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±Ù‡Ø§ Ø¯Ø± Ø¯ÙˆØ±Ø§Ù† Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø¯Ø± Ù…Ø±Ú©Ø² Ø¹Ù„Ù…ÛŒ IBM Ù†ÛŒÙˆÛŒÙˆØ±Ú© Ø¨ÙˆØ¯. Ø§Ùˆ ØªØ§Ø¨Ø³ØªØ§Ù† Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ´ØªÙ† ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ø¯Ø¯ÛŒ Ø¯Ø± Fortran Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¯Ù‡ Ø¨ÙˆØ¯. Ø§Ùˆ Ú©Ø§Ø± Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø¹Ø¯ Ø§Ø² Ø¯Ùˆ Ù‡ÙØªÙ‡ ØªÙ…Ø§Ù… Ú©Ø±Ø¯ Ùˆ Ø¨Ù‚ÛŒÙ‡ ØªØ§Ø¨Ø³ØªØ§Ù† Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ù†ÙˆØ´ØªÙ† ÛŒÚ© ÙˆÛŒØ±Ø§ÛŒØ´Ú¯Ø± Ù…ØªÙ† Ø¯Ø± APL ØµØ±Ù Ú©Ø±Ø¯. Ø§Ø³ØªØ§Ù„Ù…Ù† ØªØ§Ø¨Ø³ØªØ§Ù† Ø¨Ø¹Ø¯ Ø§Ø² ÙØ§Ø±Øºâ€ŒØ§Ù„ØªØ­ØµÛŒÙ„ÛŒâ€ŒØ§Ø´ Ø±Ø§ ØµØ±Ù Ù†ÙˆØ´ØªÙ† ÛŒÚ© Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ø²Ø¨Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ PL/I Ø¯Ø± IBM System/360 Ú©Ø±Ø¯.\n",
      "Ø¯Ø± Ø¯Ù†ÛŒØ§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ùˆ Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯ Ø¨Ù‡ Â«RMSÂ» Ø´Ù‡Ø±Øª Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯. Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ù†Ø³Ø®Ù‡ Ø§Ø² ÙØ±Ù‡Ù†Ú¯ Ù„ØºØª Ù‡Ú©Ø±Ù‡Ø§ Ø¨Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ Ú©Ù‡ Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù† Ù†Ø§Ù… Ø¯Ù†ÛŒÙˆÛŒ Ù…Ù† Ø§Ø³ØªØŒ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù† Ø±Ø§ Â«RMSÂ» Ø®Ø·Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø§Ùˆ Ø¨Ø§ PL/I Ùˆ Ø³Ù¾Ø³ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø±Ø§ÛŒØ§Ù†Ù‡ Ø¨Ø²Ø±Ú¯ Ø¨ÙˆØ¯Ù†Ø¯ Ø§Ø² Ø§Ø³Ù…Ø¨Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯. Ø¨Ø¹Ø¯ Ø§Ø² Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ùˆ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‚Ø³Ù…Øª Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø±Ø§Ú©ÙÙ„Ø± Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ Ú©Ø§Ø± Ø´Ø¯ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ùˆ Ø¨Ù‡ Ø±ÛŒØ§Ø¶ÛŒØ§Øª Ùˆ ÙÛŒØ²ÛŒÚ© ØªÙ…Ø§ÛŒÙ„ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯ØŒ ØªÙÚ©Ø± ØªØ­Ù„ÛŒÙ„ÛŒ Ø§Ùˆ Ù…Ø¯ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¯ Ù…Ø¹Ø·ÙˆÙ Ú©Ø±Ø¯ Ø¨Ù‡â€ŒØ·ÙˆØ±ÛŒâ€ŒÚ©Ù‡ Ù¾Ø³ Ø§Ø² Ú¯Ø°Ø´Øª Ù…Ø¯Øª Ø²Ù…Ø§Ù†ÛŒ Ø§Ø² Ú©Ø§Ø± Ø§Ùˆ Ø¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ù…Ø§Ø¯Ø± Ø§Ùˆ ØªÙ„ÙÙ†ÛŒ Ø§Ø² Ø³ÙˆÛŒ ÛŒÚ© Ù¾Ø±ÙˆÙØ³ÙˆØ± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ÙÙ‡Ù…ÛŒØ¯ Ú©Ù‡ RMS Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ Ú†Ù‡ Ú©Ø§Ø±ÛŒ Ø§Ø³Øª! Ø§Ùˆ Ø¯Ø± Ú©Ù…Ø§Ù„ ØªØ¹Ø¬Ø¨ Ù…ØªÙˆØ¬Ù‡ Ø´Ø¯ Ú©Ù‡ ÙˆÛŒ Ø¨Ù‡ Ø±Ø§ÛŒØ§Ù†Ù‡ Ø¹Ù„Ø§Ù‚Ù‡â€ŒÙ…Ù†Ø¯ Ø§Ø³Øª Ùˆ ÙˆÙ‚Øª Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ø¢Ù† Ù…ÛŒâ€ŒÚ¯Ø°Ø±Ø§Ù†Ø¯ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ø¯Ø±Ø®Ø´Ø§Ù†ÛŒ Ø¯Ø± Ø±Ø´ØªÙ‡ Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯. Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û·Û± Ù…ÛŒÙ„Ø§Ø¯ÛŒ ÙˆØ§Ø±Ø¯ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ù‡Ø§Ø±ÙˆØ§Ø±Ø¯ Ø´Ø¯ Ùˆ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û·Û´ Ø¨Ø§ Ù…Ø¯Ø±Ú© Ù„ÛŒØ³Ø§Ù†Ø³ Ø§Ø² Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ÙØ§Ø±Øºâ€ŒØ§Ù„ØªØ­ØµÛŒÙ„ Ø´Ø¯. Ø¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡â€ŒØ§Ù… Ø¢ÛŒ ØªÛŒ Ø¨Ù‡ ÛŒÚ© Ù‡Ú©Ø± ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯. Ø§Ùˆ Ø¨Ù‡ ÙˆØ³ÛŒÙ„Ù‡ Ø±Ø§Ø³ Ù†Ø§ÙØªØ³Ú©Ø± Ø¨Ù‡ Ú©Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. Ù…Ø±Ø¯ÛŒ Ú©Ù‡ Ø¨Ø¹Ø¯Ù‡Ø§ Ø³ÛŒÙ…Ø¨ÙˆÙ„ÛŒÚ©Ø² Ø±Ø§ Ú©Ø´Ù Ú©Ø±Ø¯ Ùˆ Ú†Ù†Ø¯ÛŒ Ø¨Ø¹Ø¯ Ø¨Ù‡ Ù…Ø®Ø§Ù„ÙØ§Ù† Ø§Ø³ØªØ§Ù„Ù…Ù† Ù¾ÛŒÙˆØ³Øª. Ø¯Ø± Ø³Ù† Û²Û± Ø³Ø§Ù„Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ø±Ú©ØªÛŒ Ø¨Ù‡ Ù†Ø§Ù… ÙˆØ³Ú†Ø³ØªØ± Ú©Ø§Ù†ØªÛŒ Ø¯Ø± Ú©Ù†Ø§Ø± Eben Moglen Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ø±Ø¯ Ú©Ù‡ Ø§Ù„Ø§Ù† ÛŒÚ© ÙˆÚ©ÛŒÙ„ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø§Ø³Øª.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Documnt Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: == Ø³Ø§Ù„â€ŒØ´Ù…Ø§Ø± Ø²Ù†Ø¯Ú¯ÛŒ ==\n",
      "\n",
      "\n",
      "=== Ø¯ÙˆØ±Ø§Ù† Ú©ÙˆØ¯Ú©ÛŒ Ùˆ Ù†ÙˆØ¬ÙˆØ§Ù†ÛŒ ===\n",
      "Ø¢Ù„ÛŒØ³ Ù„ÛŒÙ¾Ù…Ù† Ùˆ Ø¯Ø§Ù†ÛŒÙ„ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø§Ùˆ Ø±Ø§ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹ÛµÛ³ Ù…ÛŒÙ„Ø§Ø¯ÛŒ Ø¯Ø± Ø´Ù‡Ø± Ù†ÛŒÙˆÛŒÙˆØ±Ú© Ø¨Ù‡ Ø¯Ù†ÛŒØ§ Ø¢ÙˆØ±Ø¯Ù†Ø¯. Ø§ÙˆÙ„ÛŒÙ† ØªØ¬Ø±Ø¨Ù‡ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¨Ø§ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±Ù‡Ø§ Ø¯Ø± Ø¯ÙˆØ±Ø§Ù† Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø¯Ø± Ù…Ø±Ú©Ø² Ø¹Ù„Ù…ÛŒ IBM Ù†ÛŒÙˆÛŒÙˆØ±Ú© Ø¨ÙˆØ¯. Ø§Ùˆ ØªØ§Ø¨Ø³ØªØ§Ù† Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ´ØªÙ† ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ø¯Ø¯ÛŒ Ø¯Ø± Fortran Ú¯Ø°Ø±Ø§Ù†Ø¯Ù‡ Ø¨ÙˆØ¯. Ú©Ø§Ø± Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø¹Ø¯ Ø§Ø² Ø¯Ùˆ Ù‡ÙØªÙ‡ ØªÙ…Ø§Ù… Ú©Ø±Ø¯ Ùˆ Ø¨Ù‚ÛŒÙ‡ ØªØ§Ø¨Ø³ØªØ§Ù† Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ù†ÙˆØ´ØªÙ† ÛŒÚ© ÙˆÛŒØ±Ø§ÛŒØ´â€ŒÚ¯Ø± Ù…ØªÙ† Ø¯Ø± Ø§ÛŒâ€ŒÙ¾ÛŒâ€ŒØ§Ù„ (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: APL) ØµØ±Ù Ú©Ø±Ø¯.\n",
      "Û±Û¹Û·Û´: Ø¨Ø§ Ù…Ø¯Ø±Ú© Ù„ÛŒØ³Ø§Ù†Ø³ Ù‡Ù†Ø± ÙØ§Ø±Øºâ€ŒØ§Ù„ØªØ­ØµÛŒÙ„ Ø´Ø¯.\n",
      "Û±Û¹Û·Û´: Ø¯Ø± Ù‡Ø§Ø±ÙˆØ§Ø±Ø¯ Ø­Ø¶ÙˆØ± Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯.\n",
      "Û±Û¹Û·Ûµ: Ø§Ø² Ù…Ø¤Ø³Ø³Ù‡Ù” ÙÙ†Ø§ÙˆØ±ÛŒ Ù…Ø§Ø³Ø§Ú†ÙˆØ³Øª (MIT) Ø¯Ø± Ø±Ø´ØªÙ‡Ù” ÙÛŒØ²ÛŒÚ© ÙØ§Ø±Øºâ€ŒØ§Ù„ØªØ­ØµÛŒÙ„ Ø´Ø¯.\n",
      "Û±Û¹Û·Û± ØªØ§ Û±Û¹Û¸Û´: (Ø¨Ù‡ Ø¬Ø² Ø³Ø§Ù„ÛŒ Ú©Ù‡ ÛŒÚ© Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒ ÙØ§Ø±Øºâ€ŒØ§Ù„ØªØ­ØµÛŒÙ„ Ø¨ÙˆØ¯) Ø¯Ø± Ø­Ø§Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙˆØ³Ø¹Ù‡ Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„ Ø¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ù…Ø±Ú©Ø² Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ MIT Ø¨ÙˆØ¯.\n",
      "Û±Û¹Û·Û¶: Ø§ÙˆÙ„ÛŒÙ† ÙˆÛŒØ±Ø§ÛŒØ´Ú¯Ø± Ù…ØªÙ† Ø§ÛŒÙ…Ú©Ø³ (Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ: Emacs) ØªÙˆØ³Ø¹Ù‡â€ŒÙ¾Ø°ÛŒØ± Ø±Ø§ Ù†ÙˆØ´ØªØŒ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡Ù” ÛŒÚ©ÛŒ Ø§Ø² ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ (Ù†Ø§Ù… Ø¹Ù„Ù…ÛŒ: dependency-directed backtracking) Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¯Ø± Û²Û± Ø³Ø§Ù„Ú¯ÛŒ Ø¯Ø± Ú©Ø¯Ø§Ù… Ø´Ø±Ú©Øª Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ø±Ø¯ØŸ\"\n",
    "\n",
    "results = vector_store.similarity_search(query, k = 3)\n",
    "\n",
    "for result in results:\n",
    "    print('Documnt Title:', result.metadata['title'])\n",
    "    print('Document Content:', result.page_content)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e1b909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: == Ø²Ù†Ø¯Ú¯ÛŒ ==\n",
      "Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¯Ø± Ø³Ø§Ù„ Û±Û¹ÛµÛ³ Ø¯Ø± Ø´Ù‡Ø± Ù†ÛŒÙˆÛŒÙˆØ±Ú© Ø¨Ù‡ Ø¯Ù†ÛŒØ§ Ø¢Ù…Ø¯. Alice Lippman Ùˆ Daniel Stallman Ù¾Ø¯Ø± Ùˆ Ù…Ø§Ø¯Ø± Ø§Ùˆ Ø¨ÙˆØ¯Ù†Ø¯. Ø§ÙˆÙ„ÛŒÙ† ØªØ¬Ø±Ø¨Ù‡ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¨Ø§ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±Ù‡Ø§ Ø¯Ø± Ø¯ÙˆØ±Ø§Ù† Ø¯Ø¨ÛŒØ±Ø³ØªØ§Ù† Ø¯Ø± Ù…Ø±Ú©Ø² Ø¹Ù„Ù…ÛŒ IBM Ù†ÛŒÙˆÛŒÙˆØ±Ú© Ø¨ÙˆØ¯. Ø§Ùˆ ØªØ§Ø¨Ø³ØªØ§Ù† Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ´ØªÙ† ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø¢Ù†Ø§Ù„ÛŒØ² Ø¹Ø¯Ø¯ÛŒ Ø¯Ø± Fortran Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø¯Ù‡ Ø¨ÙˆØ¯. Ø§Ùˆ Ú©Ø§Ø± Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø¹Ø¯ Ø§Ø² Ø¯Ùˆ Ù‡ÙØªÙ‡ ØªÙ…Ø§Ù… Ú©Ø±Ø¯ Ùˆ Ø¨Ù‚ÛŒÙ‡ ØªØ§Ø¨Ø³ØªØ§Ù† Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ù†ÙˆØ´ØªÙ† ÛŒÚ© ÙˆÛŒØ±Ø§ÛŒØ´Ú¯Ø± Ù…ØªÙ† Ø¯Ø± APL ØµØ±Ù Ú©Ø±Ø¯. Ø§Ø³ØªØ§Ù„Ù…Ù† ØªØ§Ø¨Ø³ØªØ§Ù† Ø¨Ø¹Ø¯ Ø§Ø² ÙØ§Ø±Øºâ€ŒØ§Ù„ØªØ­ØµÛŒÙ„ÛŒâ€ŒØ§Ø´ Ø±Ø§ ØµØ±Ù Ù†ÙˆØ´ØªÙ† ÛŒÚ© Ù¾Ø±Ø¯Ø§Ø²Ø´Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ø²Ø¨Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ PL/I Ø¯Ø± IBM System/360 Ú©Ø±Ø¯.\n",
      "Ø¯Ø± Ø¯Ù†ÛŒØ§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ Ùˆ Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø§Ùˆ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯ Ø¨Ù‡ Â«RMSÂ» Ø´Ù‡Ø±Øª Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯. Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ù†Ø³Ø®Ù‡ Ø§Ø² ÙØ±Ù‡Ù†Ú¯ Ù„ØºØª Ù‡Ú©Ø±Ù‡Ø§ Ø¨Ù‡ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯ Ú©Ù‡ Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù† Ù†Ø§Ù… Ø¯Ù†ÛŒÙˆÛŒ Ù…Ù† Ø§Ø³ØªØŒ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù…Ù† Ø±Ø§ Â«RMSÂ» Ø®Ø·Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø§Ùˆ Ø¨Ø§ PL/I Ùˆ Ø³Ù¾Ø³ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø±Ø§ÛŒØ§Ù†Ù‡ Ø¨Ø²Ø±Ú¯ Ø¨ÙˆØ¯Ù†Ø¯ Ø§Ø² Ø§Ø³Ù…Ø¨Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ù‡Ø§ÛŒ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯. Ø¨Ø¹Ø¯ Ø§Ø² Ø§ÛŒÙ† Ú©Ø§Ø± Ø§Ùˆ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‚Ø³Ù…Øª Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ø±Ø§Ú©ÙÙ„Ø± Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ Ú©Ø§Ø± Ø´Ø¯ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ùˆ Ø¨Ù‡ Ø±ÛŒØ§Ø¶ÛŒØ§Øª Ùˆ ÙÛŒØ²ÛŒÚ© ØªÙ…Ø§ÛŒÙ„ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯ØŒ ØªÙÚ©Ø± ØªØ­Ù„ÛŒÙ„ÛŒ Ø§Ùˆ Ù…Ø¯ÛŒØ± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ø±Ø§ Ø¨Ù‡ Ø®ÙˆØ¯ Ù…Ø¹Ø·ÙˆÙ Ú©Ø±Ø¯ Ø¨Ù‡â€ŒØ·ÙˆØ±ÛŒâ€ŒÚ©Ù‡ Ù¾Ø³ Ø§Ø² Ú¯Ø°Ø´Øª Ù…Ø¯Øª Ø²Ù…Ø§Ù†ÛŒ Ø§Ø² Ú©Ø§Ø± Ø§Ùˆ Ø¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ØŒ Ù…Ø§Ø¯Ø± Ø§Ùˆ ØªÙ„ÙÙ†ÛŒ Ø§Ø² Ø³ÙˆÛŒ ÛŒÚ© Ù¾Ø±ÙˆÙØ³ÙˆØ± Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ÙÙ‡Ù…ÛŒØ¯ Ú©Ù‡ RMS Ù…Ø´ØºÙˆÙ„ Ø¨Ù‡ Ú†Ù‡ Ú©Ø§Ø±ÛŒ Ø§Ø³Øª! Ø§Ùˆ Ø¯Ø± Ú©Ù…Ø§Ù„ ØªØ¹Ø¬Ø¨ Ù…ØªÙˆØ¬Ù‡ Ø´Ø¯ Ú©Ù‡ ÙˆÛŒ Ø¨Ù‡ Ø±Ø§ÛŒØ§Ù†Ù‡ Ø¹Ù„Ø§Ù‚Ù‡â€ŒÙ…Ù†Ø¯ Ø§Ø³Øª Ùˆ ÙˆÙ‚Øª Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ Ø¢Ù† Ù…ÛŒâ€ŒÚ¯Ø°Ø±Ø§Ù†Ø¯ Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¢ÛŒÙ†Ø¯Ù‡ Ø¯Ø±Ø®Ø´Ø§Ù†ÛŒ Ø¯Ø± Ø±Ø´ØªÙ‡ Ø¨ÛŒÙˆÙ„ÙˆÚ˜ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ùˆ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù…ÛŒâ€ŒÚ©Ø±Ø¯. Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û·Û± Ù…ÛŒÙ„Ø§Ø¯ÛŒ ÙˆØ§Ø±Ø¯ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ Ù‡Ø§Ø±ÙˆØ§Ø±Ø¯ Ø´Ø¯ Ùˆ Ø¯Ø± Ø³Ø§Ù„ Û±Û¹Û·Û´ Ø¨Ø§ Ù…Ø¯Ø±Ú© Ù„ÛŒØ³Ø§Ù†Ø³ Ø§Ø² Ø§ÛŒÙ† Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ÙØ§Ø±Øºâ€ŒØ§Ù„ØªØ­ØµÛŒÙ„ Ø´Ø¯. Ø¯Ø± Ø¢Ø²Ù…Ø§ÛŒØ´Ú¯Ø§Ù‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡â€ŒØ§Ù… Ø¢ÛŒ ØªÛŒ Ø¨Ù‡ ÛŒÚ© Ù‡Ú©Ø± ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯. Ø§Ùˆ Ø¨Ù‡ ÙˆØ³ÛŒÙ„Ù‡ Ø±Ø§Ø³ Ù†Ø§ÙØªØ³Ú©Ø± Ø¨Ù‡ Ú©Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø´Ø¯. Ù…Ø±Ø¯ÛŒ Ú©Ù‡ Ø¨Ø¹Ø¯Ù‡Ø§ Ø³ÛŒÙ…Ø¨ÙˆÙ„ÛŒÚ©Ø² Ø±Ø§ Ú©Ø´Ù Ú©Ø±Ø¯ Ùˆ Ú†Ù†Ø¯ÛŒ Ø¨Ø¹Ø¯ Ø¨Ù‡ Ù…Ø®Ø§Ù„ÙØ§Ù† Ø§Ø³ØªØ§Ù„Ù…Ù† Ù¾ÛŒÙˆØ³Øª. Ø¯Ø± Ø³Ù† Û²Û± Ø³Ø§Ù„Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ø±Ú©ØªÛŒ Ø¨Ù‡ Ù†Ø§Ù… ÙˆØ³Ú†Ø³ØªØ± Ú©Ø§Ù†ØªÛŒ Ø¯Ø± Ú©Ù†Ø§Ø± Eben Moglen Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ø±Ø¯ Ú©Ù‡ Ø§Ù„Ø§Ù† ÛŒÚ© ÙˆÚ©ÛŒÙ„ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø§Ø³Øª.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document Title: Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù†\n",
      "Document Content: == Ø¬Ù†Ø¬Ø§Ù„ Ùˆ Ø§Ø³ØªØ¹ÙØ§ Ø§Ø² MIT Ùˆ Ø¨Ù†ÛŒØ§Ø¯ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ ==\n",
      "Ø¯Ø± Û±Û· Ø³Ù¾ØªØ§Ù…Ø¨Ø± Û²Û°Û±Û¹ Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¯Ø± Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø¬Ù†Ø¬Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´ Ø¢Ù…Ø¯Ù‡ Ù¾Ø³ Ø§Ø² Ø§Ø¸Ù‡Ø§Ø± Ù†Ø¸Ø±Ø´ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§ØªÙ‡Ø§Ù… Ø±Ø§Ø¨Ø·Ù‡ Ø¬Ù†Ø³ÛŒ Ù…Ø§Ø±ÙˆÛŒÙ† Ù…ÛŒÙ†Ø³Ú©ÛŒ (Ø§Ø³ØªØ§Ø¯ ÙÙ‚ÛŒØ¯ MIT) Ø¨Ø§ ÙˆÛŒØ±Ø¬ÛŒÙ†ÛŒØ§ Ú¯ÛŒÙˆØ±Ù‡ (ÛŒÚ©ÛŒ Ø§Ø² Ù‚Ø±Ø¨Ø§Ù†ÛŒØ§Ù† Ù¾Ø±ÙˆÙ†Ø¯Ù‡ Ø¬ÙØ±ÛŒ Ø§Ù¾Ø³ØªÛŒÙ†) Ø§Ø² MIT Ùˆ Ø¨Ù†ÛŒØ§Ø¯ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ Ø§Ø³ØªØ¹ÙØ§ Ø¯Ø§Ø¯.\n",
      "\n",
      "\n",
      "== Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø¨Ù†ÛŒØ§Ø¯ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ ==\n",
      "Ø¯Ø± Ù…Ø§Ø±Ø³ Û²Û°Û²Û±ØŒ Ø±ÛŒÚ†Ø§Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¯Ø± Ú¯ÙØªÚ¯Ùˆ Ø¨Ø§ LibrePlanet2021ØŒ Ø¨Ø§Ø²Ú¯Ø´Øª Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù‡ Ø¨Ù†ÛŒØ§Ø¯ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± Ø¢Ø²Ø§Ø¯ (FSF) Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯.\n",
      "\n",
      "\n",
      "== Ø¬ÙˆØ§ÛŒØ² Ùˆ Ø§ÙØªØ®Ø§Ø±Ø§Øª ==\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = 'Ø±ÛŒÚ†Ø§Ø±Ø¯ Ø§Ø³ØªØ§Ù„Ù…Ù† Ø¯Ø± Û²Û± Ø³Ø§Ù„Ú¯ÛŒ Ø¯Ø± Ú©Ø¯Ø§Ù… Ø´Ø±Ú©Øª Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ø±Ø¯ØŸ'\n",
    "\n",
    "results = vector_store.max_marginal_relevance_search(query, fetch_k=8, k=3)\n",
    "\n",
    "for result in results:\n",
    "    print('Document Title:', result.metadata['title'])\n",
    "    print('Document Content:', result.page_content)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d72c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name = \"Released_Year\",\n",
    "        description = \"The year the movie was released. You must put Released_Year in quotes like \\\"Released_Year\\\"\",\n",
    "        type = \"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name = \"IMDB_Rating\",\n",
    "        description = \"A 1-10 rating for the movie. You must put IMDB_Rating in quotes like \\\"IMDB_Rating\\\"\",\n",
    "        type = \"float\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatCohere(temperature=0)\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1154e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia documents in 'fa' language...\n",
      "Loading document for query: 'Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ'\n",
      "âœ“ Successfully loaded 1 document(s) for 'Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ'\n",
      "ðŸ” ANALYZING WIKIPEDIA DOCUMENT STRUCTURE\n",
      "==================================================\n",
      "âœ“ Split 1 documents into 2 sections by headers\n",
      "\n",
      "ðŸ“‘ DOCUMENT STRUCTURE: Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ\n",
      "============================================================\n",
      "\n",
      "# Ø§Ù‡Ø¯Ø§Ù (50 words)\n",
      "  ## Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ùˆ Ø­Ù„ Ù…Ø³Ø¦Ù„Ù‡ (45 words)\n",
      "\n",
      "ðŸ“Š SECTION DETAILS:\n",
      "Total sections found: 2\n",
      "\n",
      "--- Section 1 ---\n",
      "Header: Ø§Ù‡Ø¯Ø§Ù (Level 1)\n",
      "Word count: 50\n",
      "Content preview: Ù…Ø³Ø¦Ù„Ù‡ Ú©Ù„ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ (ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯) Ù‡ÙˆØ´ Ø¨Ù‡ Ø²ÛŒØ±Ù…Ø³Ø¦Ù„Ù‡â€ŒÙ‡Ø§ÛŒÛŒ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ø²ÛŒØ±Ù…Ø³Ø¦Ù„Ù‡â€ŒÙ‡Ø§ Ø´Ø§Ù…Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÛŒØ§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§ØµÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ ÛŒÚ©...\n",
      "\n",
      "--- Section 2 ---\n",
      "Header: Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ùˆ Ø­Ù„ Ù…Ø³Ø¦Ù„Ù‡ (Level 2)\n",
      "Word count: 45\n",
      "Content preview: Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù† Ø§ÙˆÙ„ÛŒÙ‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒÛŒ Ø±Ø§ ØªÙˆØ³Ø¹Ù‡ Ø¯Ø§Ø¯Ù†Ø¯ Ú©Ù‡ Ø§Ø² Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú¯Ø§Ù… Ø¨Ù‡ Ú¯Ø§Ù…ÛŒ Ú©Ù‡ Ø§Ù†Ø³Ø§Ù†â€ŒÙ‡Ø§ Ù‡Ù†Ú¯Ø§Ù… Ø­Ù„ Ù…Ø¹Ù…Ø§Ù‡Ø§ ÛŒØ§ Ø§Ù†Ø¬Ø§Ù… Ø§Ø³ØªÙ†ØªØ§Ø¬â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø·Ù‚ÛŒ Ø¨Ù‡ Ú©Ø§Ø± Ù…ÛŒâ€ŒØ¨Ø±Ù†Ø¯ØŒ ØªÙ‚Ù„ÛŒØ¯ Ù…ÛŒ...\n",
      "\n",
      "ðŸ”„ SPLITTING LARGE SECTIONS:\n",
      "âœ“ Created 2 total chunks (including split large sections)\n",
      "Total chunks after splitting: 2\n",
      "\n",
      "ðŸ“„ EXPORTING TO LANGCHAIN DOCUMENTS:\n",
      "Created 2 LangChain Document objects\n",
      "\n",
      "First document preview:\n",
      "Content: Section: Ø§Ù‡Ø¯Ø§Ù\n",
      "\n",
      "Ù…Ø³Ø¦Ù„Ù‡ Ú©Ù„ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ (ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯) Ù‡ÙˆØ´ Ø¨Ù‡ Ø²ÛŒØ±Ù…Ø³Ø¦Ù„Ù‡â€ŒÙ‡Ø§ÛŒÛŒ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ø²ÛŒØ±Ù…Ø³Ø¦Ù„Ù‡â€ŒÙ‡Ø§ Ø´Ø§Ù…Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÛŒØ§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø®Ø§ØµÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ù¾Ú˜ÙˆÙ‡Ø´Ú¯Ø±Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ ÛŒÚ© Ø³ÛŒØ³ØªÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø§Ø² Ø®ÙˆØ¯ Ù†Ø´Ø§Ù† Ø¯Ù‡Ø¯. ÙˆÛŒÚ˜...\n",
      "Metadata: {'source': 'wikipedia', 'title': 'Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ', 'header': 'Ø§Ù‡Ø¯Ø§Ù', 'level': 1, 'section_index': 0, 'word_count': 50, 'char_count': 285, 'language': 'fa'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "\n",
    "class WikipediaHeaderSplitter:\n",
    "    \"\"\"\n",
    "    A class to load and split Wikipedia documents by their headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, queries: List[str], load_max_docs: int = 1, lang: str = 'fa'):\n",
    "        \"\"\"\n",
    "        Initialize the Wikipedia header splitter.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of search queries for Wikipedia\n",
    "            load_max_docs: Maximum number of documents to load per query\n",
    "            lang: Language code for Wikipedia (default: 'fa' for Persian)\n",
    "        \"\"\"\n",
    "        self.queries = queries\n",
    "        self.load_max_docs = load_max_docs\n",
    "        self.lang = lang\n",
    "        self.raw_documents = []\n",
    "        self.header_documents = []\n",
    "        \n",
    "        # Load documents\n",
    "        self.load_documents()\n",
    "    \n",
    "    def load_documents(self) -> None:\n",
    "        \"\"\"Load documents from Wikipedia for each query.\"\"\"\n",
    "        print(f\"Loading Wikipedia documents in '{self.lang}' language...\")\n",
    "        \n",
    "        for query in self.queries:\n",
    "            try:\n",
    "                print(f\"Loading document for query: '{query}'\")\n",
    "                \n",
    "                loader = WikipediaLoader(\n",
    "                    query=query,\n",
    "                    load_max_docs=self.load_max_docs,\n",
    "                    lang=self.lang\n",
    "                )\n",
    "                \n",
    "                docs = loader.load()\n",
    "                self.raw_documents.extend(docs)\n",
    "                print(f\"âœ“ Successfully loaded {len(docs)} document(s) for '{query}'\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error loading document for '{query}': {e}\")\n",
    "    \n",
    "    def split_by_headers(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Split Wikipedia documents by headers.\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing header, content, and metadata\n",
    "        \"\"\"\n",
    "        self.header_documents = []\n",
    "        \n",
    "        for doc in self.raw_documents:\n",
    "            sections = self._parse_wikipedia_sections(doc.page_content)\n",
    "            self.header_documents.extend(sections)\n",
    "        \n",
    "        print(f\"âœ“ Split {len(self.raw_documents)} documents into {len(self.header_documents)} sections by headers\")\n",
    "        return self.header_documents\n",
    "    \n",
    "    def _parse_wikipedia_sections(self, content: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Parse Wikipedia content and split by headers/sections.\n",
    "        \n",
    "        Args:\n",
    "            content: Raw Wikipedia content\n",
    "            \n",
    "        Returns:\n",
    "            List of section dictionaries\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Pattern for Wikipedia headers (== Header ==, === Subheader ===, etc.)\n",
    "        # This works for both English and Persian Wikipedia\n",
    "        header_pattern = r'(\\n|^)(={2,})\\s*(.*?)\\s*\\2(\\n|$)'\n",
    "        \n",
    "        # Find all headers and their positions\n",
    "        matches = list(re.finditer(header_pattern, content))\n",
    "        \n",
    "        if not matches:\n",
    "            # If no headers found, treat entire content as one section\n",
    "            sections.append({\n",
    "                'header': 'Introduction',\n",
    "                'content': content.strip(),\n",
    "                'level': 1,\n",
    "                'word_count': len(content.split())\n",
    "            })\n",
    "            return sections\n",
    "        \n",
    "        # Process each section\n",
    "        for i, match in enumerate(matches):\n",
    "            header_level = len(match.group(2)) - 1  # == is level 1, === is level 2, etc.\n",
    "            header_text = match.group(3).strip()\n",
    "            \n",
    "            # Determine content start and end positions\n",
    "            content_start = match.end()\n",
    "            \n",
    "            if i < len(matches) - 1:\n",
    "                content_end = matches[i + 1].start()\n",
    "            else:\n",
    "                content_end = len(content)\n",
    "            \n",
    "            section_content = content[content_start:content_end].strip()\n",
    "            \n",
    "            # Only add if content is not empty\n",
    "            if section_content:\n",
    "                sections.append({\n",
    "                    'header': header_text,\n",
    "                    'content': section_content,\n",
    "                    'level': header_level,\n",
    "                    'word_count': len(section_content.split()),\n",
    "                    'char_count': len(section_content)\n",
    "                })\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def get_hierarchical_structure(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get hierarchical structure of the document.\n",
    "        \n",
    "        Returns:\n",
    "            Nested dictionary representing document hierarchy\n",
    "        \"\"\"\n",
    "        if not self.header_documents:\n",
    "            self.split_by_headers()\n",
    "        \n",
    "        hierarchy = {\n",
    "            'title': self.raw_documents[0].metadata['title'] if self.raw_documents else 'Unknown',\n",
    "            'sections': []\n",
    "        }\n",
    "        \n",
    "        current_level_1 = None\n",
    "        current_level_2 = None\n",
    "        \n",
    "        for section in self.header_documents:\n",
    "            section_data = {\n",
    "                'header': section['header'],\n",
    "                'content_preview': section['content'][:100] + '...',\n",
    "                'word_count': section['word_count'],\n",
    "                'level': section['level']\n",
    "            }\n",
    "            \n",
    "            if section['level'] == 1:\n",
    "                current_level_1 = {'section': section_data, 'subsections': []}\n",
    "                hierarchy['sections'].append(current_level_1)\n",
    "                current_level_2 = None\n",
    "            elif section['level'] == 2 and current_level_1:\n",
    "                current_level_2 = {'section': section_data, 'subsections': []}\n",
    "                current_level_1['subsections'].append(current_level_2)\n",
    "            elif section['level'] == 3 and current_level_2:\n",
    "                current_level_2['subsections'].append(section_data)\n",
    "        \n",
    "        return hierarchy\n",
    "    \n",
    "    def split_large_sections(self, max_chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Further split large sections using recursive text splitting.\n",
    "        \n",
    "        Args:\n",
    "            max_chunk_size: Maximum chunk size for large sections\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of all chunks (both header-based and split large sections)\n",
    "        \"\"\"\n",
    "        if not self.header_documents:\n",
    "            self.split_by_headers()\n",
    "        \n",
    "        all_chunks = []\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        for section in self.header_documents:\n",
    "            # If section is small, keep it as is\n",
    "            if len(section['content']) <= max_chunk_size:\n",
    "                all_chunks.append({\n",
    "                    'header': section['header'],\n",
    "                    'content': section['content'],\n",
    "                    'level': section['level'],\n",
    "                    'is_split': False,\n",
    "                    'chunk_index': 0,\n",
    "                    'total_chunks': 1\n",
    "                })\n",
    "            else:\n",
    "                # Split large sections\n",
    "                chunks = text_splitter.split_text(section['content'])\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    all_chunks.append({\n",
    "                        'header': section['header'],\n",
    "                        'content': chunk,\n",
    "                        'level': section['level'],\n",
    "                        'is_split': True,\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(chunks)\n",
    "                    })\n",
    "        \n",
    "        print(f\"âœ“ Created {len(all_chunks)} total chunks (including split large sections)\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def print_document_structure(self) -> None:\n",
    "        \"\"\"Print the hierarchical structure of the document.\"\"\"\n",
    "        hierarchy = self.get_hierarchical_structure()\n",
    "        \n",
    "        print(f\"\\nðŸ“‘ DOCUMENT STRUCTURE: {hierarchy['title']}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for section in hierarchy['sections']:\n",
    "            level1 = section['section']\n",
    "            print(f\"\\n# {level1['header']} ({level1['word_count']} words)\")\n",
    "            \n",
    "            for subsection in section['subsections']:\n",
    "                level2 = subsection['section']\n",
    "                print(f\"  ## {level2['header']} ({level2['word_count']} words)\")\n",
    "                \n",
    "                for subsubsection in subsection['subsections']:\n",
    "                    level3 = subsubsection\n",
    "                    print(f\"    ### {level3['header']} ({level3['word_count']} words)\")\n",
    "    \n",
    "    def export_to_langchain_documents(self) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Convert header-based chunks to LangChain Document format.\n",
    "        \n",
    "        Returns:\n",
    "            List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        from langchain.schema import Document\n",
    "        \n",
    "        if not self.header_documents:\n",
    "            self.split_by_headers()\n",
    "        \n",
    "        langchain_docs = []\n",
    "        \n",
    "        for i, section in enumerate(self.header_documents):\n",
    "            # Create enhanced metadata\n",
    "            metadata = {\n",
    "                'source': 'wikipedia',\n",
    "                'title': self.raw_documents[0].metadata['title'] if self.raw_documents else 'Unknown',\n",
    "                'header': section['header'],\n",
    "                'level': section['level'],\n",
    "                'section_index': i,\n",
    "                'word_count': section['word_count'],\n",
    "                'char_count': section['char_count'],\n",
    "                'language': self.lang\n",
    "            }\n",
    "            \n",
    "            # Combine header and content for better context\n",
    "            enhanced_content = f\"Section: {section['header']}\\n\\n{section['content']}\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=enhanced_content,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            langchain_docs.append(doc)\n",
    "        \n",
    "        return langchain_docs\n",
    "\n",
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Example queries for Persian Wikipedia\n",
    "    queries = []\n",
    "    \n",
    "    # Create header splitter\n",
    "    splitter = WikipediaHeaderSplitter(\n",
    "        queries=queries,\n",
    "        load_max_docs=1,\n",
    "        lang='fa'\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ” ANALYZING WIKIPEDIA DOCUMENT STRUCTURE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split by headers\n",
    "    sections = splitter.split_by_headers()\n",
    "    \n",
    "    # Print document structure\n",
    "    splitter.print_document_structure()\n",
    "    \n",
    "    # Show section details\n",
    "    print(f\"\\nðŸ“Š SECTION DETAILS:\")\n",
    "    print(f\"Total sections found: {len(sections)}\")\n",
    "    \n",
    "    for i, section in enumerate(sections[:5]):  # Show first 5 sections\n",
    "        print(f\"\\n--- Section {i+1} ---\")\n",
    "        print(f\"Header: {section['header']} (Level {section['level']})\")\n",
    "        print(f\"Word count: {section['word_count']}\")\n",
    "        print(f\"Content preview: {section['content'][:150]}...\")\n",
    "    \n",
    "    # Further split large sections if needed\n",
    "    print(f\"\\nðŸ”„ SPLITTING LARGE SECTIONS:\")\n",
    "    all_chunks = splitter.split_large_sections(max_chunk_size=800, chunk_overlap=100)\n",
    "    \n",
    "    print(f\"Total chunks after splitting: {len(all_chunks)}\")\n",
    "    \n",
    "    # Show split chunks\n",
    "    split_chunks = [chunk for chunk in all_chunks if chunk['is_split']]\n",
    "    if split_chunks:\n",
    "        print(f\"Split chunks: {len(split_chunks)}\")\n",
    "        for chunk in split_chunks[:3]:\n",
    "            print(f\"  - {chunk['header']} [Part {chunk['chunk_index'] + 1}/{chunk['total_chunks']}]\")\n",
    "    \n",
    "    # Export to LangChain documents\n",
    "    print(f\"\\nðŸ“„ EXPORTING TO LANGCHAIN DOCUMENTS:\")\n",
    "    langchain_docs = splitter.export_to_langchain_documents()\n",
    "    print(f\"Created {len(langchain_docs)} LangChain Document objects\")\n",
    "    \n",
    "    # Show first document as example\n",
    "    if langchain_docs:\n",
    "        first_doc = langchain_docs[0]\n",
    "        print(f\"\\nFirst document preview:\")\n",
    "        print(f\"Content: {first_doc.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {first_doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7974f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "\n",
    "class WikipediaHeaderSplitter:\n",
    "    \"\"\"\n",
    "    A class to load and split Wikipedia documents by their headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, queries: List[str], load_max_docs: int = 1, lang: str = 'fa'):\n",
    "        \"\"\"\n",
    "        Initialize the Wikipedia header splitter.\n",
    "        \n",
    "        Args:\n",
    "            queries: List of search queries for Wikipedia\n",
    "            load_max_docs: Maximum number of documents to load per query\n",
    "            lang: Language code for Wikipedia (default: 'fa' for Persian)\n",
    "        \"\"\"\n",
    "        self.queries = queries\n",
    "        self.load_max_docs = load_max_docs\n",
    "        self.lang = lang\n",
    "        self.raw_documents = []\n",
    "        self.header_documents = []\n",
    "        \n",
    "        # Load documents\n",
    "        self.load_documents()\n",
    "    \n",
    "    def load_documents(self) -> None:\n",
    "        \"\"\"Load documents from Wikipedia for each query.\"\"\"\n",
    "        print(f\"Loading Wikipedia documents in '{self.lang}' language...\")\n",
    "        \n",
    "        for query in self.queries:\n",
    "            try:\n",
    "                print(f\"Loading document for query: '{query}'\")\n",
    "                \n",
    "                loader = WikipediaLoader(\n",
    "                    query=query,\n",
    "                    load_max_docs=self.load_max_docs,\n",
    "                    lang=self.lang\n",
    "                )\n",
    "                \n",
    "                docs = loader.load()\n",
    "                self.raw_documents.extend(docs)\n",
    "                print(f\"âœ“ Successfully loaded {len(docs)} document(s) for '{query}'\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âœ— Error loading document for '{query}': {e}\")\n",
    "    \n",
    "    def split_by_headers(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Split Wikipedia documents by headers.\n",
    "        \n",
    "        Returns:\n",
    "            List of dictionaries containing header, content, and metadata\n",
    "        \"\"\"\n",
    "        self.header_documents = []\n",
    "        \n",
    "        for doc in self.raw_documents:\n",
    "            sections = self._parse_wikipedia_sections(doc.page_content)\n",
    "            self.header_documents.extend(sections)\n",
    "        \n",
    "        print(f\"âœ“ Split {len(self.raw_documents)} documents into {len(self.header_documents)} sections by headers\")\n",
    "        return self.header_documents\n",
    "    \n",
    "    def _parse_wikipedia_sections(self, content: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Parse Wikipedia content and split by headers/sections.\n",
    "        \n",
    "        Args:\n",
    "            content: Raw Wikipedia content\n",
    "            \n",
    "        Returns:\n",
    "            List of section dictionaries\n",
    "        \"\"\"\n",
    "        sections = []\n",
    "        \n",
    "        # Pattern for Wikipedia headers (== Header ==, === Subheader ===, etc.)\n",
    "        # This works for both English and Persian Wikipedia\n",
    "        header_pattern = r'(\\n|^)(={2,})\\s*(.*?)\\s*\\2(\\n|$)'\n",
    "        \n",
    "        # Find all headers and their positions\n",
    "        matches = list(re.finditer(header_pattern, content))\n",
    "        \n",
    "        if not matches:\n",
    "            # If no headers found, treat entire content as one section\n",
    "            sections.append({\n",
    "                'header': 'Introduction',\n",
    "                'content': content.strip(),\n",
    "                'level': 1,\n",
    "                'word_count': len(content.split())\n",
    "            })\n",
    "            return sections\n",
    "        \n",
    "        # Process each section\n",
    "        for i, match in enumerate(matches):\n",
    "            header_level = len(match.group(2)) - 1  # == is level 1, === is level 2, etc.\n",
    "            header_text = match.group(3).strip()\n",
    "            \n",
    "            # Determine content start and end positions\n",
    "            content_start = match.end()\n",
    "            \n",
    "            if i < len(matches) - 1:\n",
    "                content_end = matches[i + 1].start()\n",
    "            else:\n",
    "                content_end = len(content)\n",
    "            \n",
    "            section_content = content[content_start:content_end].strip()\n",
    "            \n",
    "            # Only add if content is not empty\n",
    "            if section_content:\n",
    "                sections.append({\n",
    "                    'header': header_text,\n",
    "                    'content': section_content,\n",
    "                    'level': header_level,\n",
    "                    'word_count': len(section_content.split()),\n",
    "                    'char_count': len(section_content)\n",
    "                })\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def get_hierarchical_structure(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get hierarchical structure of the document.\n",
    "        \n",
    "        Returns:\n",
    "            Nested dictionary representing document hierarchy\n",
    "        \"\"\"\n",
    "        if not self.header_documents:\n",
    "            self.split_by_headers()\n",
    "        \n",
    "        hierarchy = {\n",
    "            'title': self.raw_documents[0].metadata['title'] if self.raw_documents else 'Unknown',\n",
    "            'sections': []\n",
    "        }\n",
    "        \n",
    "        current_level_1 = None\n",
    "        current_level_2 = None\n",
    "        \n",
    "        for section in self.header_documents:\n",
    "            section_data = {\n",
    "                'header': section['header'],\n",
    "                'content_preview': section['content'][:100] + '...',\n",
    "                'word_count': section['word_count'],\n",
    "                'level': section['level']\n",
    "            }\n",
    "            \n",
    "            if section['level'] == 1:\n",
    "                current_level_1 = {'section': section_data, 'subsections': []}\n",
    "                hierarchy['sections'].append(current_level_1)\n",
    "                current_level_2 = None\n",
    "            elif section['level'] == 2 and current_level_1:\n",
    "                current_level_2 = {'section': section_data, 'subsections': []}\n",
    "                current_level_1['subsections'].append(current_level_2)\n",
    "            elif section['level'] == 3 and current_level_2:\n",
    "                current_level_2['subsections'].append(section_data)\n",
    "        \n",
    "        return hierarchy\n",
    "    \n",
    "    def split_large_sections(self, max_chunk_size: int = 1000, chunk_overlap: int = 200) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Further split large sections using recursive text splitting.\n",
    "        \n",
    "        Args:\n",
    "            max_chunk_size: Maximum chunk size for large sections\n",
    "            chunk_overlap: Overlap between chunks\n",
    "            \n",
    "        Returns:\n",
    "            List of all chunks (both header-based and split large sections)\n",
    "        \"\"\"\n",
    "        if not self.header_documents:\n",
    "            self.split_by_headers()\n",
    "        \n",
    "        all_chunks = []\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=max_chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        for section in self.header_documents:\n",
    "            # If section is small, keep it as is\n",
    "            if len(section['content']) <= max_chunk_size:\n",
    "                all_chunks.append({\n",
    "                    'header': section['header'],\n",
    "                    'content': section['content'],\n",
    "                    'level': section['level'],\n",
    "                    'is_split': False,\n",
    "                    'chunk_index': 0,\n",
    "                    'total_chunks': 1\n",
    "                })\n",
    "            else:\n",
    "                # Split large sections\n",
    "                chunks = text_splitter.split_text(section['content'])\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    all_chunks.append({\n",
    "                        'header': section['header'],\n",
    "                        'content': chunk,\n",
    "                        'level': section['level'],\n",
    "                        'is_split': True,\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(chunks)\n",
    "                    })\n",
    "        \n",
    "        print(f\"âœ“ Created {len(all_chunks)} total chunks (including split large sections)\")\n",
    "        return all_chunks\n",
    "    \n",
    "    def print_document_structure(self) -> None:\n",
    "        \"\"\"Print the hierarchical structure of the document.\"\"\"\n",
    "        hierarchy = self.get_hierarchical_structure()\n",
    "        \n",
    "        print(f\"\\nðŸ“‘ DOCUMENT STRUCTURE: {hierarchy['title']}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for section in hierarchy['sections']:\n",
    "            level1 = section['section']\n",
    "            print(f\"\\n# {level1['header']} ({level1['word_count']} words)\")\n",
    "            \n",
    "            for subsection in section['subsections']:\n",
    "                level2 = subsection['section']\n",
    "                print(f\"  ## {level2['header']} ({level2['word_count']} words)\")\n",
    "                \n",
    "                for subsubsection in subsection['subsections']:\n",
    "                    level3 = subsubsection\n",
    "                    print(f\"    ### {level3['header']} ({level3['word_count']} words)\")\n",
    "    \n",
    "    def export_to_langchain_documents(self) -> List[Any]:\n",
    "        \"\"\"\n",
    "        Convert header-based chunks to LangChain Document format.\n",
    "        \n",
    "        Returns:\n",
    "            List of LangChain Document objects\n",
    "        \"\"\"\n",
    "        from langchain.schema import Document\n",
    "        \n",
    "        if not self.header_documents:\n",
    "            self.split_by_headers()\n",
    "        \n",
    "        langchain_docs = []\n",
    "        \n",
    "        for i, section in enumerate(self.header_documents):\n",
    "            # Create enhanced metadata\n",
    "            metadata = {\n",
    "                'source': 'wikipedia',\n",
    "                'title': self.raw_documents[0].metadata['title'] if self.raw_documents else 'Unknown',\n",
    "                'header': section['header'],\n",
    "                'level': section['level'],\n",
    "                'section_index': i,\n",
    "                'word_count': section['word_count'],\n",
    "                'char_count': section['char_count'],\n",
    "                'language': self.lang\n",
    "            }\n",
    "            \n",
    "            # Combine header and content for better context\n",
    "            enhanced_content = f\"Section: {section['header']}\\n\\n{section['content']}\"\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=enhanced_content,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            langchain_docs.append(doc)\n",
    "        \n",
    "        return langchain_docs\n",
    "\n",
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Example queries for Persian Wikipedia\n",
    "    queries = [\"Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ\"]\n",
    "    \n",
    "    # Create header splitter\n",
    "    splitter = WikipediaHeaderSplitter(\n",
    "        queries=queries,\n",
    "        load_max_docs=1,\n",
    "        lang='fa'\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸ” ANALYZING WIKIPEDIA DOCUMENT STRUCTURE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split by headers\n",
    "    sections = splitter.split_by_headers()\n",
    "    \n",
    "    # Print document structure\n",
    "    splitter.print_document_structure()\n",
    "    \n",
    "    # Show section details\n",
    "    print(f\"\\nðŸ“Š SECTION DETAILS:\")\n",
    "    print(f\"Total sections found: {len(sections)}\")\n",
    "    \n",
    "    for i, section in enumerate(sections[:5]):  # Show first 5 sections\n",
    "        print(f\"\\n--- Section {i+1} ---\")\n",
    "        print(f\"Header: {section['header']} (Level {section['level']})\")\n",
    "        print(f\"Word count: {section['word_count']}\")\n",
    "        print(f\"Content preview: {section['content'][:150]}...\")\n",
    "    \n",
    "    # Further split large sections if needed\n",
    "    print(f\"\\nðŸ”„ SPLITTING LARGE SECTIONS:\")\n",
    "    all_chunks = splitter.split_large_sections(max_chunk_size=800, chunk_overlap=100)\n",
    "    \n",
    "    print(f\"Total chunks after splitting: {len(all_chunks)}\")\n",
    "    \n",
    "    # Show split chunks\n",
    "    split_chunks = [chunk for chunk in all_chunks if chunk['is_split']]\n",
    "    if split_chunks:\n",
    "        print(f\"Split chunks: {len(split_chunks)}\")\n",
    "        for chunk in split_chunks[:3]:\n",
    "            print(f\"  - {chunk['header']} [Part {chunk['chunk_index'] + 1}/{chunk['total_chunks']}]\")\n",
    "    \n",
    "    # Export to LangChain documents\n",
    "    print(f\"\\nðŸ“„ EXPORTING TO LANGCHAIN DOCUMENTS:\")\n",
    "    langchain_docs = splitter.export_to_langchain_documents()\n",
    "    print(f\"Created {len(langchain_docs)} LangChain Document objects\")\n",
    "    \n",
    "    # Show first document as example\n",
    "    if langchain_docs:\n",
    "        first_doc = langchain_docs[0]\n",
    "        print(f\"\\nFirst document preview:\")\n",
    "        print(f\"Content: {first_doc.page_content[:200]}...\")\n",
    "        print(f\"Metadata: {first_doc.metadata}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
